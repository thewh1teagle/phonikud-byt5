#!/usr/bin/env python3

from optimum.onnxruntime import ORTModelForSeq2SeqLM
from transformers import ByT5Tokenizer
from pathlib import Path
from tap import Tap


class PredictArgs(Tap):
    model_path: str = "./onnx_model"  # Path to ONNX model directory
    max_length: int = 512  # Maximum sequence length for generation
    num_beams: int = 4  # Number of beams for beam search


def main():
    """Run inference with ONNX ByT5 model"""
    args = PredictArgs().parse_args()
    
    print(f"ğŸ”® ByT5 ONNX Inference using HuggingFace Optimum")
    print(f"   Model: {args.model_path}")
    print(f"   Max length: {args.max_length}")
    print(f"   Num beams: {args.num_beams}")
    
    # Hebrew text to predict (same as original)
    hebrew_text = "×‘×•× × ×©×—×§ ×‘×™×¨×•×©×œ×™×! ×‘×•× ×ª×¨×“ ×œ××›×•×œ ×ª×¨×“! ×”×•× ×¨×¦×” ××ª ×–×” ×’× ××‘×œ ×”×™× ×¨×¦×” ××”×¨ ×•×”×§×“×™××” ××•×ª×•!"
    
    # Check if model exists
    model_path = Path(args.model_path)
    if not model_path.exists():
        print(f"âŒ Model directory not found: {model_path}")
        print(f"   Run export.py first to create the ONNX model")
        return
    
    # Check for ONNX model
    onnx_files = list(model_path.glob("*.onnx"))
    if not onnx_files:
        print(f"âŒ No ONNX files found in {model_path}")
        print(f"   Run export.py first to create the ONNX model")
        return
    
    try:
        print(f"ğŸ“¦ Loading ONNX model and tokenizer...")
        
        # Load the ONNX model using Optimum
        model = ORTModelForSeq2SeqLM.from_pretrained(str(model_path))
        tokenizer = ByT5Tokenizer.from_pretrained(str(model_path))
        
        print(f"âœ… Model and tokenizer loaded successfully")
        print(f"   ONNX Runtime providers: {model.providers}")
        
        print(f"\nğŸ”® Predicting phonemes for Hebrew text...")
        print(f"Input: {hebrew_text}")
        
        # Tokenize input
        inputs = tokenizer(
            hebrew_text,
            max_length=args.max_length,
            padding=True,
            truncation=True,
            return_tensors="pt"
        )
        
        print(f"ğŸ”„ Running inference...")
        
        # Generate prediction using the ONNX model
        # Optimum makes this exactly the same as using the original PyTorch model
        outputs = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_length=args.max_length,
            num_beams=args.num_beams,
            do_sample=False,
            early_stopping=True,
        )
        
        # Decode the prediction
        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        print(f"âœ… Inference completed successfully")
        print(f"\nğŸ“ Results:")
        print(f"Phonemes: {prediction}")
        
        print(f"\nâš¡ Performance Info:")
        print(f"   - Using ONNX Runtime for faster inference")
        print(f"   - Optimized memory usage compared to PyTorch")
        print(f"   - Same generation quality as original model")
        
    except Exception as e:
        print(f"âŒ Error during prediction: {e}")
        print(f"\nTroubleshooting:")
        print(f"   1. Make sure export.py completed successfully")
        print(f"   2. Check that ONNX files exist in {model_path}")
        print(f"   3. Try: pip install --upgrade optimum[onnxruntime]")


if __name__ == "__main__":
    main()
